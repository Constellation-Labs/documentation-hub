---
title: Private vs Public
slug: private-public
hide_table_of_contents: false
---

The decision to create a private state channel network versus a public one will really depend on the business case which can evolve over time. For example, a private state channel might decide to own and operate all of its validator nodes. This of course comes with the overhead of paying out of pocket to own, manage, and maintain hardware resources.

Determining the degree of decentralization and permissions that a state channel needs will likely be influenced by the consumers of the data. A **private** state channel may decide to whitelist a select group of trusted node operators, while a **public** one may allow anyone to be a node operator.

## Ethical considerations

Let's say an IoT sensor company decides to incorporate Web3 business logic into their enterprise by tokenizing the sensor data so they can sell it to other businesses or organizations. Would this change the expectation of the state channel configuration to one that is permissionless rather than private? In the Web 2.0 world, a consumer places their trust in a business's reputation and brand when making the decision to engage in procurement of goods and services. Will this paradigm extend into tokenized datasets and data streams? This ultimately depends on what the data will be used for and what consequences bad data will have on the individual, business, or organization consuming the data. Is it worth the risk or would the consumer prefer that the data be validated by a large network of publicly owned and operated validator nodes which are not potentially subject to unilateral manipulation (private) or collusion (Permissioned) to spoof/alter state data?

What if the data is used to train machine learning models where the provenance and integrity of the data is paramount? For the model to be successful it must avoid the "garbage in; garbage out" dilemma, which will negatively impact the business's product or service quality, its reputation, and ultimately its revenues. In the worst-case scenario, bad data could impact a critical outcome that literally results in human death and suffering. This could manifest as miscalculations in the prediction algorithms of an autonomous vehicle's computer vision system, corruption of data that is used for sensitive manufacturing processes, disruption of physical supply chain which can result in spoiled medical and food supplies, and poisoning of the code supply chain which could have disastrous effects on software development life cycles. Automation presents unprecedented opportunities to introduce efficiency into existing business logic as well as innovate entirely new business paradigms through tokenization models, however, this can only be achieved if decisions and insights derived from data pipelines are founded in high fidelity data that can be trusted.